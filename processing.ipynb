{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"processing.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNtNR7fgVk6LLjpXksOkfii"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WJdtV0FLSC-N","colab_type":"text"},"source":["The output of this file is training and testing data that has been cleaned and is ready to use. Long articles have beeen omitted and the remaining ones have been truncated to 300 token. Additionally the gold standard summaries have beeen limited to 75 tokens."]},{"cell_type":"code","metadata":{"id":"WNF950wFUvAw","colab_type":"code","outputId":"f2069389-0661-4e0d-d126-7e7b265991c2","executionInfo":{"status":"ok","timestamp":1587911731629,"user_tz":240,"elapsed":1275,"user":{"displayName":"Iain Graham","photoUrl":"","userId":"00727570204428341338"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd \"/content/drive/My Drive/630Project\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/My Drive/630Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2qZHJC2eU2n0","colab_type":"code","colab":{}},"source":["import re\n","import pickle\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncXWIxWeVC8q","colab_type":"code","colab":{}},"source":["data = pickle.load(open('all_article_info.pkl', 'rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pquAgQNMEf01","colab_type":"text"},"source":["Creating dictionaries of the articles to work with"]},{"cell_type":"code","metadata":{"id":"u4c8U_R8VEM7","colab_type":"code","colab":{}},"source":["full_article_dict = {}\n","full_summary_dict = {}\n","\n","i = 0\n","for x in data:\n","  summary = x['article']\n","  if summary[:3] == 'cnn':\n","    summary = summary[3:]\n","  full_article_dict[i] = summary\n","  full_summary_dict[i] = x['gold_summary']\n","  i+=1\n","\n","article_dict = {}\n","summary_dict = {}\n","\n","i = 0\n","for x in data:\n","  summary = x['article']\n","  if summary[:3] == 'cnn':\n","    summary = summary[3:]\n","  article_dict[i] = summary\n","  summary_dict[i] = x['gold_summary']\n","  i+=1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNsenODkEod2","colab_type":"text"},"source":["Processing the articles/summaries by adding space between punctuation, replacing all numbers with #, and adding start and end tokens"]},{"cell_type":"code","metadata":{"id":"KTVnuvl8VEPD","colab_type":"code","colab":{}},"source":["def preprocess_sentence(w):\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    w = re.sub('\\d', '#', w)\n","    # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","    w = w.strip()\n","\n","    w = '<start> ' + w + ' <end>'\n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrZvDj1YVERd","colab_type":"code","colab":{}},"source":["for x in range(len(article_dict)):\n","  article_dict[x] = preprocess_sentence(article_dict[x])\n","  summary_dict[x] = preprocess_sentence(summary_dict[x])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ZF8wvRBjyWV","colab_type":"code","outputId":"8952b499-1bc6-477d-ab22-0100e9d68cab","executionInfo":{"status":"ok","timestamp":1587911857499,"user_tz":240,"elapsed":120413,"user":{"displayName":"Iain Graham","photoUrl":"","userId":"00727570204428341338"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(article_dict))\n","print(len(summary_dict))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["311971\n","311971\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kLccilDrPxeC","colab_type":"text"},"source":["Here, we are cleaning up the articles a little moree by dropping those that start with 'by'. Thes articles have a header block that adds uneeded information to the model"]},{"cell_type":"code","metadata":{"id":"os5kvUTePwy9","colab_type":"code","colab":{}},"source":["i=0\n","by_list = []\n","for article in list(article_dict.values()):\n","  if article.split()[1] == 'by':\n","    del article_dict[i]\n","    del summary_dict[i]\n","  i+=1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdvgQ_RHPEx6","colab_type":"text"},"source":["We are getting rid of long articles here (> 500 tokens)"]},{"cell_type":"code","metadata":{"id":"9kiL6iZ6VETx","colab_type":"code","colab":{}},"source":["keys = list(article_dict.keys())\n","\n","for x in keys:\n","  if len(article_dict[x].split())>500:\n","    del article_dict[x]\n","    del summary_dict[x]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fwBLmc9lIXO","colab_type":"code","outputId":"ce321d6f-5d14-4fcc-b056-2df4da8b6425","executionInfo":{"status":"ok","timestamp":1587911878014,"user_tz":240,"elapsed":135878,"user":{"displayName":"Iain Graham","photoUrl":"","userId":"00727570204428341338"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(article_dict))\n","print(len(summary_dict))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["70305\n","70305\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fwu0M2iVE84C","colab_type":"text"},"source":["Following steps close to See et. al. I am truncating articles and summaries under the assumption that most important information is at the beginning of the article and to make training less computationally expensive"]},{"cell_type":"code","metadata":{"id":"PtDXA_3AbzFM","colab_type":"code","colab":{}},"source":["keys = list(article_dict.keys())\n","\n","for x in keys:\n","  article_dict[x] = ' '.join(article_dict[x].split()[:250])\n","  summary_dict[x] = ' '.join(summary_dict[x].split()[:75])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ly_O6QJ3IHCN","colab_type":"text"},"source":["Now we ar making word count dictionaries so that we can replace uncommon words with unk token"]},{"cell_type":"code","metadata":{"id":"QPyR2jk0VEY0","colab_type":"code","colab":{}},"source":["articles = list(article_dict.values())\n","summaries = list(summary_dict.values())\n","\n","freq_dict_articles = {}\n","\n","for article in articles:\n","    tokens = article.split()\n","    for token in tokens:\n","        if token in freq_dict_articles:\n","            freq_dict_articles[token] += 1\n","        else:\n","            freq_dict_articles[token] = 1\n","\n","freq_dict_summaries = {}\n","\n","for summary in summaries:\n","    tokens = summary.split()\n","    for token in tokens:\n","        if token in freq_dict_summaries:\n","            freq_dict_summaries[token] += 1\n","        else:\n","            freq_dict_summaries[token] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRrWCNR8MmwA","colab_type":"code","colab":{}},"source":["keys = list(article_dict.keys())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3YENFG9NXIw","colab_type":"text"},"source":["Now we are replacing the rare words (in this case, words that only happen once) with the unk token"]},{"cell_type":"code","metadata":{"id":"lB88hpQHVEbW","colab_type":"code","colab":{}},"source":["new_articles = []\n","for article in articles:\n","    temp = []\n","    tokens = article.split()\n","    for token in tokens:\n","      if freq_dict_articles[token]>30:\n","        temp.append(token)\n","      else:\n","        temp.append('<unk>')\n","\n","    new = \" \".join(temp)\n","    new_articles.append(new)\n","\n","\n","new_summaries = []\n","for summary in summaries:\n","    temp = []\n","    tokens = summary.split()\n","    for token in tokens:\n","      if freq_dict_summaries[token]>20:\n","        temp.append(token)\n","      else:\n","        temp.append('<unk>')\n","\n","    new = \" \".join(temp)\n","    new_summaries.append(new)\n","\n","\n","i = 0\n","for x in keys:\n","  article_dict[x] = new_articles[i]\n","  summary_dict[x] = new_summaries[i]\n","  i+=1\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"943wp6N2RDZc","colab_type":"text"},"source":["Here we get rid of article/summary combos that contain a lot of unknown tokens"]},{"cell_type":"code","metadata":{"id":"aNpplFnuVEd5","colab_type":"code","outputId":"344db0cd-2f84-4726-ab8f-bd12eb1a763a","executionInfo":{"status":"ok","timestamp":1587911893680,"user_tz":240,"elapsed":15626,"user":{"displayName":"Iain Graham","photoUrl":"","userId":"00727570204428341338"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["print('Length of article and summary dicts before filtering:\\n')\n","print(len(article_dict))\n","print(len(summary_dict))\n","print()\n","\n","for x in keys:\n","  # print(x)\n","  article = article_dict[x]\n","  summary = summary_dict[x]\n","  if article.count('<unk>') > 3 and summary.count('<unk>') > 2:\n","    del article_dict[x]\n","    del summary_dict[x]\n","\n","print('Length of article and summary dicts after filtering ones with a lot of rare words:\\n')\n","print(len(article_dict))\n","print(len(summary_dict))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Length of article and summary dicts before filtering:\n","\n","70305\n","70305\n","\n","Length of article and summary dicts after filtering ones with a lot of rare words:\n","\n","34540\n","34540\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ejgrIyG2mc5J","colab_type":"code","outputId":"825c75e2-14ef-4264-924f-d1d247b65ace","executionInfo":{"status":"ok","timestamp":1587911893681,"user_tz":240,"elapsed":15621,"user":{"displayName":"Iain Graham","photoUrl":"","userId":"00727570204428341338"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["keys = list(article_dict.keys())\n","print(keys[:10])\n","random.shuffle(keys)\n","print(keys[:10])\n","\n","filtered_articles = []\n","filtered_summaries = []\n","\n","for x in keys:\n","  filtered_articles.append(article_dict[x])\n","  filtered_summaries.append(summary_dict[x])\n","\n","\n","train_pct_index = int(0.9 * len(keys))\n","X_train, X_test = filtered_articles[:train_pct_index], filtered_articles[train_pct_index:]\n","y_train, y_test = filtered_summaries[:train_pct_index], filtered_summaries[train_pct_index:]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[7, 10, 13, 14, 15, 22, 29, 35, 47, 48]\n","[4085, 215621, 288211, 79841, 123883, 21292, 37063, 56451, 14447, 224167]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vD11_rYio_Xy","colab_type":"code","colab":{}},"source":["with open(\"x_train_filtered.txt\", \"w\") as filehandle:\n","    for listitem in X_train:\n","        filehandle.write('%s\\n' % listitem)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRTLYR7HmdAP","colab_type":"code","colab":{}},"source":["with open(\"x_test_filtered.txt\", \"w\") as filehandle:\n","    for listitem in X_test:\n","        filehandle.write('%s\\n' % listitem)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yv8tJp_7ppaW","colab_type":"code","colab":{}},"source":["with open(\"y_train_filtered.txt\", \"w\") as filehandle:\n","    for listitem in y_train:\n","        filehandle.write('%s\\n' % listitem)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyKzLWXappfY","colab_type":"code","colab":{}},"source":["with open(\"y_test_filtered.txt\", \"w\") as filehandle:\n","    for listitem in y_test:\n","        filehandle.write('%s\\n' % listitem)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyWX148AzF83","colab_type":"code","colab":{}},"source":["filtered_articles = []\n","filtered_summaries = []\n","\n","for x in keys:\n","  filtered_articles.append(full_article_dict[x])\n","  filtered_summaries.append(full_summary_dict[x])\n","\n","\n","train_pct_index = int(0.8 * len(keys))\n","X_train_full, X_test_full = filtered_articles[:train_pct_index], filtered_articles[train_pct_index:]\n","y_train_full, y_test_full = filtered_summaries[:train_pct_index], filtered_summaries[train_pct_index:]\n","\n","with open(\"X_train_full.txt\", \"w\") as filehandle:\n","    for listitem in X_train_full:\n","        filehandle.write('%s\\n' % listitem)\n","\n","\n","with open(\"X_test_full.txt\", \"w\") as filehandle:\n","    for listitem in X_test_full:\n","        filehandle.write('%s\\n' % listitem)\n","\n","with open(\"y_train_full.txt\", \"w\") as filehandle:\n","    for listitem in y_train_full:\n","        filehandle.write('%s\\n' % listitem)\n","\n","with open(\"y_test_full.txt\", \"w\") as filehandle:\n","    for listitem in y_test_full:\n","        filehandle.write('%s\\n' % listitem)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJyIPi8BBxwM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}